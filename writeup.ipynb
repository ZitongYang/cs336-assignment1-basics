{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Understanding Unicode\n",
    "\n",
    "* (a) `\\x00`\n",
    "* (b) The printed representation is white space.\n",
    "* (c) When string is evaluated, it appear as `\\x00`. When the string is printed, it appear as white space.\n",
    "\n",
    "### 2. Unicode Encodings\n",
    "* (a) UTF-16 or -32 encode all unicode into at least 2 or 4 bytes. But UTF-8 includes some unicode to a single byte. This makes the output byte sequence shorter.\n",
    "* (b) `'ðŸ˜§'.encode('utf-8')`. We should decode by iterate over the bytes, there are some cases where a multiple bytes corresponds to one unicode string.\n",
    "* (c) `'\\xC0\\xC0'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. BPE Training on TinyStories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) My `tinystories` take `0.23 hours`. According to `wandb`, it uses `8GB` peak memory and only `0.3GB` long term memory. The longest token is `b' accomplishments'`.\n",
    "* (b) The longest part is when I call length function to access if my vocab length has met the goal. The next most time consuming part is when I call the `max` function to find the most frequent pair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. BPE Training on OpenWebText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) The longest voacb is\n",
    "```\n",
    "b'\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82\\xc3\\x83\\xc3\\x82'\n",
    "```\n",
    "* (b) The vocabulary for OpenWebText is much more complex than tinystories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Experiments with tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "(cs336_basics) [zitong@sh03-18n13 /home/groups/candes/zitong/cs336-assignment1-basics] (job 44602574) $ python cs336_basics/scripts/tokenizer_experiments.py \n",
    "--------------------------------------------------\n",
    "Sample text from TinyStory:\n",
    "\n",
    "Once upon a time there was a little boy named Ben ...\n",
    "Estimated compression ratio for TinyStory: 1.7282659228265922\n",
    "Estimated throughput for TinyStory: 75830.04308877455 bytes/sec\n",
    "--------------------------------------------------\n",
    "Sample text from OpenWebText:\n",
    "What wouldn't you do to save someone you love?\n",
    "\n",
    "Wh ...\n",
    "Estimated compression ratio for OpenWebText: 4.7143284922892645\n",
    "Estimated throughput for OpenWebText: 74177.9909660446 bytes/sec\n",
    "--------------------------------------------------\n",
    "Estimated compression ratio for OpenWebText using TinyStory tokenizer: 1.7201311117181097\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) The compression ratio for TinyStories is `1.7282659228265922` and for OpenWebText is `4.7143284922892645`.\n",
    "* (b) If we use TinyStories tokenizer for OpenWebText, the compression ratio is `1.7201311117181097`.\n",
    "* (c) The throughput for TinyStories is `75830.04308877455 bytes/sec` and for OpenWebText is `74177.9909660446 bytes/sec`. If we use this throughput on Pile with 825GB of text, it will take `(8.25e+11/741778)/3600=308` hours.\n",
    "* (d) `uint16` is an appropriate choice as it is unsigned, so we don't waste a bit on the sign. It is also 2 bytes long, which is enough to store the maximum value of 65,535, whereas the vocabulary size is 32,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Transformer LM resource accounting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters in GPT2-XL: 1637176000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 50257\n",
    "context_length = 1024\n",
    "num_layers = 48\n",
    "d_model = 1600\n",
    "num_heads = 25\n",
    "d_ff = 6400\n",
    "\n",
    "# calculate the number of trainable parameters in the model\n",
    "embedding = vocab_size * d_model + context_length * d_model\n",
    "multihead = 3 * d_model * d_model + d_model * d_model\n",
    "ffn = d_model * d_ff + d_ff * d_model\n",
    "transformer_block = d_model + multihead + d_model + ffn\n",
    "lm_head = d_model * vocab_size\n",
    "transformer_lm = embedding + num_layers * transformer_block + d_model + lm_head\n",
    "print(f\"Number of trainable parameters in GPT2-XL: {transformer_lm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory accounting\n",
    "\n",
    "| Model   | Vocab Size | Context Length | Num Layers | d_model | d_ff | Embedding | FFN        | Attn       | Total Param | Percent FFN | Percent Attn | Percent Emb  | Memory (GB) |\n",
    "|---------|------------|----------------|------------|---------|------|-----------|------------|------------|-------------|-------------|--------------|--------------|-------------|\n",
    "| GPT2-XL | 50257      | 1024           | 48         | 1600    | 6400 | 162462400 | 983116800  | 491596800  | 1637176000  | 0.600495487 | 0.300271199  | 0.099233314  | 6.548704    |\n",
    "| GPT2-L  | 50257      | 1024           | 36         | 1280    | 5120 | 129969920 | 471905280  | 235975680  | 837850880   | 0.563233018 | 0.281644008  | 0.155122974  | 3.35140352  |\n",
    "| GPT2-M  | 50257      | 1024           | 24         | 1024    | 4096 | 103975936 | 201351168  | 100687872  | 406014976   | 0.495920545 | 0.247990537  | 0.256088918  | 1.624059904 |\n",
    "| GPT2-S  | 50257      | 1024           | 12         | 768     | 3072 | 77981952  | 56632320   | 28320768   | 162935040   | 0.347576065 | 0.173816314  | 0.478607622  | 0.65174016  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (a) See above. Memory size is `(1637176000*4)/(10**9)=6.5 GB`. See below for FLOPs accounting. \n",
    "* (b) The emebdding layer requires most FLOPs.\n",
    "* (c) As the model size increases, the fraction of FLOPSs taken by MLP increases most dramatically.\n",
    "* (d) With longer context, attention begin to take more FLOPs then FFN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FLOPs accounting\n",
    "\n",
    "| Model   | Vocab Size | Context Length | Num Layers | d_model | d_ff  | emb_FLOPs    | ffn_FLOPs  | attn_FLOPs | total_FLOPs  | Percent Emb  | Percent FFN  | Percent Attn |\n",
    "|---------|------------|----------------|------------|---------|-------|--------------|------------|------------|--------------|--------------|--------------|--------------|\n",
    "| GPT2-XL | 50257      | 1024           | 48         | 1600    | 6400  | 164682137600 | 41943040000| 22439526400 | 229064704000 | 0.718932837  | 0.183105643  | 0.097961519  |\n",
    "| GPT2-L  | 50257      | 1024           | 36         | 1280    | 5120  | 131745710080 | 26843545600| 15435038720 | 174024294400 | 0.757053551  | 0.154251713  | 0.088694735  |\n",
    "| GPT2-M  | 50257      | 1024           | 24         | 1024    | 4096  | 105396568064 | 17179869184| 10737418240 | 133313855488 | 0.790589753  | 0.128867844  | 0.080542403  |\n",
    "| GPT2-S  | 50257      | 1024           | 12         | 768     | 3072  | 79047426048  | 9663676416 | 6845104128  | 95556206592  | 0.827234869  | 0.101130808  | 0.071634323  |\n",
    "|GPT2-XL-Long-Context|\t50257|\t16384|\t48|\t1600|\t6400|\t2634914201600|\t671088640000|\t1969645158400|\t5275648000000|\t0.499448447204969|\t0.127204968944099|\t0.373346583850932|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Tuning the learning rate\n",
    "\n",
    "With outrageously high learning rate, the losses diverges. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Resource accounting for training with AdamW\n",
    "\n",
    "* (a) Memory accounting for training with AdamW: Let `num_heads`=$H$, `d_model`=$d$, `bath_size`=$B$, `context_length`=$L$, `vocab_size`=$V$, `num_layers`=$N$. According to the table below, the total memory required is $N[6d+3BHL^2+46d^2+21BLd]+V(BL+4d)+BL$.\n",
    "\n",
    "|                 | Copies | Parameters | Activation | Gradients | Adam States | Total           |\n",
    "|-----------------|--------|------------|------------|-----------|-------------|-----------------|\n",
    "| RMSNorm         | $2N+1$ | $d$        | $BLd$      | $d$       | $2d$        | $d(2N+1)(BL+3)$ |\n",
    "| $Q/K/V$         | $N$    | 3d^2       | $3BLd$     | $3d^2$    | $6d^2$      | $N(12d^2+3BLd)$ |\n",
    "| $QK^T$          | $N$    | $0$        | $BHL^2$    | $0$       | $0$         | $NBHL^2$        |\n",
    "| `softmax`       | $N$    | $0$        | $BHL^2$    | $0$       | $0$         | $NBHL^2$        |\n",
    "| `weight_sum`    | $N$    | $0$        | $BLd$      | $0$       | $0$         | $NBHL^2$        |\n",
    "| `projection`    | $N$    | $d^2$      | $BLd$      | $d^2$     | $2d^2$      | $N(2d^2+4BLd)$  |\n",
    "| $W_1$           | $N$    | $4d^2$     | $4BLd$     | $4d^2$    | $8d^2$      | $N(4BLd+16d^2)$ |\n",
    "| `GeLU`          | $N$    | $0$        | $4BLd$     | $0$       | $0$         | $4NBLd$         |\n",
    "| $W_2$           | $N$    | $4d^2$     | $4BLd$     | $4d^2$    | $8d^2$      | $N(4BLd+16d^2)$ |\n",
    "| `lm_head`       | $1$    | $Vd$       | $BLV$      | $Vd$      | $2Vd$       | $BLV+4Vd$       |\n",
    "| `cross_entropy` | $1$    | $0$        | $BL$       | $0$       | $0$         | $BL$            |\n",
    "\n",
    "* (b) The number of floating point needed to strore in memory is $B[3NHL^2+21NLd+L(d+V+1)]+(46Nd^2+6Nd+4Vd+3d)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size of B require B * 0.82284544 GB + 23.8983616 GB of memory.\n",
      "On a single GPU with 80GB SRAM, the maximum batch size is 68.\n"
     ]
    }
   ],
   "source": [
    "a = 3*num_layers*(context_length**2)+21*num_layers*d_model+context_length + context_length*(d_model+vocab_size+1)\n",
    "b = 46*num_layers*(d_model**2)+6*num_layers*d_model+4*vocab_size*d_model+3*d_model\n",
    "print(f'Batch size of B require B * {a*4/(10**9)} GB + {b*4/(10**9)} GB of memory.')\n",
    "print(f'On a single GPU with 80GB SRAM, the maximum batch size is {int((80 - b*4/(10**9))/(a*4/(10**9)))}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (c) According to the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336_basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
